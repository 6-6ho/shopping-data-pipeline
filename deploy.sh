#!/bin/bash
# =============================================================================
# Shopping Data Pipeline - Automated Deployment Script
# =============================================================================
# Usage: ./deploy.sh [OPTIONS]
# Options:
#   --full      : Full deployment (build + start all services)
#   --start     : Start services only (assumes images exist)
#   --stop      : Stop all services
#   --clean     : Stop and remove all containers, volumes, networks
#   --status    : Show status of all services
# =============================================================================

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DOCKER_DIR="${SCRIPT_DIR}/docker"
ENV_FILE="${DOCKER_DIR}/.env"

# =============================================================================
# Helper Functions
# =============================================================================

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

check_docker() {
    log_info "Checking Docker installation..."
    if ! command -v docker &> /dev/null; then
        log_error "Docker is not installed. Please install Docker first."
        exit 1
    fi
    
    if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
        log_error "Docker Compose is not installed. Please install Docker Compose first."
        exit 1
    fi
    
    log_success "Docker is installed: $(docker --version)"
}

get_host_ip() {
    # Try to get the primary IP address
    if command -v hostname &> /dev/null; then
        hostname -I 2>/dev/null | awk '{print $1}' || echo "localhost"
    else
        echo "localhost"
    fi
}

configure_env() {
    log_info "Configuring environment..."
    
    HOST_IP=$(get_host_ip)
    
    # Create or update .env file
    cat > "${ENV_FILE}" << EOF
# Auto-generated by deploy.sh
HOST_IP=${HOST_IP}
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
EOF
    
    log_success "Environment configured with HOST_IP=${HOST_IP}"
}

# =============================================================================
# Service Management
# =============================================================================

start_core_services() {
    log_info "Starting core services (Kafka, MinIO, Spark, Airflow, Grafana)..."
    
    cd "${DOCKER_DIR}"
    
    # Start core services
    docker compose up -d zookeeper kafka minio minio-init
    log_info "Waiting for Kafka to be ready..."
    sleep 10
    
    # Start Spark cluster
    docker compose up -d spark-master spark-worker-1
    log_info "Waiting for Spark cluster..."
    sleep 5
    
    # Start Postgres and Airflow
    docker compose up -d postgres
    sleep 5
    docker compose up -d airflow-init
    log_info "Waiting for Airflow initialization..."
    sleep 20
    docker compose up -d airflow-webserver airflow-scheduler
    
    # Start Grafana
    docker compose up -d grafana
    
    # Start Prometheus (monitoring profile)
    docker compose --profile monitoring up -d prometheus
    
    log_success "Core services started!"
}

start_data_generator() {
    log_info "Starting data generator..."
    
    cd "${DOCKER_DIR}"
    docker compose --profile generator up -d data-generator
    
    log_success "Data generator started!"
}

start_streaming_jobs() {
    log_info "Starting Spark streaming jobs..."
    
    # Wait for Spark to be ready
    sleep 10
    
    # Start shopping events streaming
    docker exec -d spark-master bash -c "/opt/spark/bin/spark-submit \
        --master local[2] \
        --conf spark.jars.ivy=/tmp/ivy \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
        /opt/spark-jobs/streaming/shopping_stream.py > /tmp/shopping_stream.log 2>&1"
    
    # Start reviews streaming
    docker exec -d spark-master bash -c "/opt/spark/bin/spark-submit \
        --master local[2] \
        --conf spark.jars.ivy=/tmp/ivy \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
        /opt/spark-jobs/streaming/reviews_stream.py > /tmp/reviews_stream.log 2>&1"

    # Start anomaly detection streaming
    docker exec -d spark-master bash -c "/opt/spark/bin/spark-submit \
        --master local[2] \
        --conf spark.jars.ivy=/tmp/ivy \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
        /opt/spark-jobs/streaming/anomaly_detection.py > /tmp/anomaly_detection.log 2>&1"
    
    log_success "Streaming jobs started!"
}

stop_all() {
    log_info "Stopping all services..."
    
    cd "${DOCKER_DIR}"
    docker compose --profile generator --profile monitoring --profile scale down
    
    log_success "All services stopped!"
}

clean_all() {
    log_info "Cleaning up all containers, volumes, and networks..."
    
    cd "${DOCKER_DIR}"
    docker compose --profile generator --profile monitoring --profile scale down -v --remove-orphans
    
    log_success "Cleanup complete!"
}

show_status() {
    log_info "Service Status:"
    echo ""
    
    cd "${DOCKER_DIR}"
    docker compose ps
    
    echo ""
    log_info "Access URLs:"
    HOST_IP=$(get_host_ip)
    echo "  - Grafana:      http://${HOST_IP}:3000 (admin/admin)"
    echo "  - Spark Master: http://${HOST_IP}:8080"
    echo "  - Airflow:      http://${HOST_IP}:8090 (admin/admin)"
    echo "  - MinIO:        http://${HOST_IP}:9001 (minioadmin/minioadmin123)"
    echo "  - Prometheus:   http://${HOST_IP}:9091"
}

run_batch_jobs() {
    log_info "Running batch aggregation jobs..."
    
    TODAY=$(date +%Y-%m-%d)
    
    # Daily Aggregation
    docker exec spark-master /opt/spark/bin/spark-submit \
        --master local[*] \
        --conf spark.jars.ivy=/tmp/ivy \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
        /opt/spark-jobs/batch/daily_aggregation.py ${TODAY}
    
    log_success "Daily aggregation completed!"

    # Cohort Analysis
    docker exec spark-master /opt/spark/bin/spark-submit \
        --master local[*] \
        --conf spark.jars.ivy=/tmp/ivy \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
        /opt/spark-jobs/batch/cohort_analysis.py ${TODAY}
    
    log_success "Cohort analysis completed!"
    
    # Review Analysis (if reviews exist)
    docker exec spark-master /opt/spark/bin/spark-submit \
        --master local[*] \
        --conf spark.jars.ivy=/tmp/ivy \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
        /opt/spark-jobs/batch/review_analysis.py ${TODAY} || log_warning "Review analysis skipped (no data)"
    
    log_success "Batch jobs completed!"
}

# =============================================================================
# Main Deployment
# =============================================================================

full_deploy() {
    log_info "=========================================="
    log_info "  Shopping Data Pipeline - Full Deploy"
    log_info "=========================================="
    
    check_docker
    configure_env
    
    log_info "Building Docker images..."
    cd "${DOCKER_DIR}"
    docker compose build
    
    start_core_services
    start_data_generator
    
    log_info "Waiting for data to accumulate (30 seconds)..."
    sleep 30
    
    start_streaming_jobs
    
    echo ""
    log_success "=========================================="
    log_success "  Deployment Complete!"
    log_success "=========================================="
    
    show_status
}

# =============================================================================
# Command Line Interface
# =============================================================================

case "${1:-}" in
    --full)
        full_deploy
        ;;
    --start)
        check_docker
        start_core_services
        start_data_generator
        start_streaming_jobs
        show_status
        ;;
    --stop)
        stop_all
        ;;
    --clean)
        clean_all
        ;;
    --status)
        show_status
        ;;
    --batch)
        run_batch_jobs
        ;;
    --streaming)
        start_streaming_jobs
        ;;
    *)
        echo "Usage: $0 [OPTIONS]"
        echo ""
        echo "Options:"
        echo "  --full       Full deployment (build + start all services)"
        echo "  --start      Start services only"
        echo "  --stop       Stop all services"
        echo "  --clean      Stop and remove all containers, volumes"
        echo "  --status     Show status of all services"
        echo "  --batch      Run batch aggregation jobs"
        echo "  --streaming  Start streaming jobs only"
        echo ""
        echo "Example:"
        echo "  $0 --full    # First-time deployment"
        echo "  $0 --status  # Check service status"
        exit 1
        ;;
esac
