version: '3.8'

services:
  # ============== Zookeeper & Kafka ==============
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - pipeline-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # Using variable HOST_IP with default or localhost fallback if not set. 
      # For the user's server, this will be 192.168.219.101 via .env or shell variable
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://${HOST_IP:-192.168.219.101}:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - pipeline-network

  # ============== MinIO (S3 Compatible Storage) ==============
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9003:9000"
      - "9002:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-minioadmin123}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - pipeline-network

  minio-init:
    image: minio/mc
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      until mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER:-minioadmin} ${MINIO_ROOT_PASSWORD:-minioadmin123}; do
        echo 'Waiting for MinIO...'
        sleep 5
      done;
      mc mb myminio/raw --ignore-existing;
      mc mb myminio/processed --ignore-existing;
      mc mb myminio/aggregated --ignore-existing;
      mc mb myminio/iceberg-warehouse --ignore-existing;
      mc mb myminio/checklist-data || true;
      echo 'Buckets created successfully';
      exit 0;
      "
    networks:
      - pipeline-network

  # ============== Spark Cluster ==============
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    ports:
      - "8088:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKER_MEMORY=2g
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD:-minioadmin123}
      - AWS_REGION=us-east-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - ../spark-jobs:/opt/spark-jobs
      - spark-logs:/opt/spark/spark-events
    networks:
      - pipeline-network

  spark-worker-1:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD:-minioadmin123}
      - AWS_REGION=us-east-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ../spark-jobs:/opt/spark-jobs
    networks:
      - pipeline-network

  spark-worker-2:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8082"
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8082
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD:-minioadmin123}
      - AWS_REGION=us-east-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ../spark-jobs:/opt/spark-jobs
    networks:
      - pipeline-network
    profiles:
      - scale

  spark-worker-3:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-3
    depends_on:
      - spark-master
    ports:
      - "8083:8083"
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8083
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD:-minioadmin123}
      - AWS_REGION=us-east-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ../spark-jobs:/opt/spark-jobs
    networks:
      - pipeline-network
    profiles:
      - scale

  spark-worker-4:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-4
    depends_on:
      - spark-master
    ports:
      - "8084:8084"
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8084
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD:-minioadmin123}
      - AWS_REGION=us-east-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ../spark-jobs:/opt/spark-jobs
    networks:
      - pipeline-network
    profiles:
      - scale

  # ============== Airflow ==============
  postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - pipeline-network

  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=uf-JR5_b0p-7xWdvf0FS-5q-NXmEfrQqqOx8UHVWnCc=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    networks:
      - pipeline-network

  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    depends_on:
      - postgres
      - airflow-init
    ports:
      - "8090:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=uf-JR5_b0p-7xWdvf0FS-5q-NXmEfrQqqOx8UHVWnCc=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    volumes:
      - ../airflow/dags:/opt/airflow/dags
    command: webserver
    networks:
      - pipeline-network

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=uf-JR5_b0p-7xWdvf0FS-5q-NXmEfrQqqOx8UHVWnCc=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ../airflow/dags:/opt/airflow/dags
    command: scheduler
    networks:
      - pipeline-network

  # ============== Grafana ==============
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3002:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana-data:/var/lib/grafana
      - ../grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - pipeline-network

  # ============== Data Generator ==============
  data-generator:
    build:
      context: ../data-generator
      dockerfile: Dockerfile
    container_name: data-generator
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - MODE=normal
      # Inject HOST_IP so the generator knows about the environment if needed
      - HOST_IP=${HOST_IP:-192.168.219.101}
    volumes:
      - ../data-generator:/app
    networks:
      - pipeline-network
    profiles:
      - generator

  # ============== Monitoring (Optional) ==============
  # Added as per improvement plan request for better observability
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - pipeline-network
    profiles:
      - monitoring

  # ============== Dashboard ==============
  dashboard:
    build:
      context: ../dashboard
      dockerfile: Dockerfile
    container_name: dashboard
    ports:
      - "8501:8501"
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-airflow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-airflow}
      - POSTGRES_DB=airflow
    depends_on:
      - postgres
    networks:
      - pipeline-network

  # ============== Nginx (Reverse Proxy) ==============
  nginx:
    image: nginx:latest
    container_name: nginx-proxy
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - dashboard
    networks:
      - pipeline-network

networks:
  pipeline-network:
    driver: bridge

volumes:
  minio-data:
  postgres-data:
  grafana-data:
  spark-logs:
